@misc{szegedy2014intriguing,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Biggio_2013,
   title={Evasion Attacks against Machine Learning at Test Time},
   ISBN={9783642387098},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-642-40994-3_25},
   DOI={10.1007/978-3-642-40994-3_25},
   journal={Lecture Notes in Computer Science},
   publisher={Springer Berlin Heidelberg},
   author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Šrndić, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
   year={2013},
   pages={387–402}
}

@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples}, 
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{kurakin2017adversarial,
      title={Adversarial examples in the physical world}, 
      author={Alexey Kurakin and Ian Goodfellow and Samy Bengio},
      year={2017},
      eprint={1607.02533},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{madry2019deep,
      title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
      author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
      year={2019},
      eprint={1706.06083},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{papernot2015limitations,
      title={The Limitations of Deep Learning in Adversarial Settings}, 
      author={Nicolas Papernot and Patrick McDaniel and Somesh Jha and Matt Fredrikson and Z. Berkay Celik and Ananthram Swami},
      year={2015},
      eprint={1511.07528},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{carlini2017evaluating,
      title={Towards Evaluating the Robustness of Neural Networks}, 
      author={Nicholas Carlini and David Wagner},
      year={2017},
      eprint={1608.04644},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{papernot2017practical,
      title={Practical Black-Box Attacks against Machine Learning}, 
      author={Nicolas Papernot and Patrick McDaniel and Ian Goodfellow and Somesh Jha and Z. Berkay Celik and Ananthram Swami},
      year={2017},
      eprint={1602.02697},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{brendel2018decisionbased,
      title={Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models}, 
      author={Wieland Brendel and Jonas Rauber and Matthias Bethge},
      year={2018},
      eprint={1712.04248},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{ilyas2018blackbox,
      title={Black-box Adversarial Attacks with Limited Queries and Information}, 
      author={Andrew Ilyas and Logan Engstrom and Anish Athalye and Jessy Lin},
      year={2018},
      eprint={1804.08598},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{liu2017delving,
      title={Delving into Transferable Adversarial Examples and Black-box Attacks}, 
      author={Yanpei Liu and Xinyun Chen and Chang Liu and Dawn Song},
      year={2017},
      eprint={1611.02770},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dong2017interpretable,
      title={Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples}, 
      author={Yinpeng Dong and Hang Su and Jun Zhu and Fan Bao},
      year={2017},
      eprint={1708.05493},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2019interpreting,
      title={Interpreting and Improving Adversarial Robustness of Deep Neural Networks with Neuron Sensitivity}, 
      author={Chongzhi Zhang and Aishan Liu and Xianglong Liu and Yitao Xu and Hang Yu and Yuqing Ma and Tianlin Li},
      year={2019},
      eprint={1909.06978},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{kurakin2017scale,
      title={Adversarial Machine Learning at Scale}, 
      author={Alexey Kurakin and Ian Goodfellow and Samy Bengio},
      year={2017},
      eprint={1611.01236},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wu2020skip,
      title={Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets}, 
      author={Dongxian Wu and Yisen Wang and Shu-Tao Xia and James Bailey and Xingjun Ma},
      year={2020},
      eprint={2002.05990},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2018interpreting,
      title={Interpreting Adversarial Robustness: A View from Decision Surface in Input Space}, 
      author={Fuxun Yu and Chenchen Liu and Yanzhi Wang and Liang Zhao and Xiang Chen},
      year={2018},
      eprint={1810.00144},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shan2020fawkes,
      title={Fawkes: Protecting Privacy against Unauthorized Deep Learning Models}, 
      author={Shawn Shan and Emily Wenger and Jiayun Zhang and Huiying Li and Haitao Zheng and Ben Y. Zhao},
      year={2020},
      eprint={2002.08327},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{demontis2019adversarial,
      title={Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks}, 
      author={Ambra Demontis and Marco Melis and Maura Pintor and Matthew Jagielski and Battista Biggio and Alina Oprea and Cristina Nita-Rotaru and Fabio Roli},
      year={2019},
      eprint={1809.02861},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{pmlr-v129-wu20a, title = {Towards Understanding and Improving the Transferability of Adversarial Examples in Deep Neural Networks}, author = {Wu, Lei and Zhu, Zhanxing}, pages = {837--850}, year = {2020}, editor = {Sinno Jialin Pan and Masashi Sugiyama}, volume = {129}, series = {Proceedings of Machine Learning Research}, address = {Bangkok, Thailand}, month = {18--20 Nov}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v129/wu20a/wu20a.pdf}, url = {http://proceedings.mlr.press/v129/wu20a.html}, abstract = {Currently it is well known that deep neural networks are vulnerable to adversarial examples, constructed by applying small but malicious perturbations to the original inputs. Moreover, the perturbed inputs can transfer between different models: adversarial examples generated based on a specific model will often fool other unseen models with a significant success rate. This allows the adversary to leverage it to attack the deployed systems without any query, which could raise severe security issue particularly in safety-critical scenarios. In this work, we empirically investigate two classes of factors that might influence the transferability of adversarial examples. One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for generating adversarial examples. More importantly, relying on these findings on the transferability of adversarial examples, we propose a simple but effective strategy to improve the transferability, whose effectiveness is confirmed through extensive experiments on both CIFAR-10 and ImageNet datasets.} }

@misc{noack2020training,
      title={Training Deep Neural Networks for Interpretability and Adversarial Robustness}, 
      author={Adam Noack and Isaac Ahern and Dejing Dou and Boyang Li},
      year={2020},
      eprint={1912.03430},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ortizjimenez2020optimism,
      title={Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness}, 
      author={Guillermo Ortiz-Jimenez and Apostolos Modas and Seyed-Mohsen Moosavi-Dezfooli and Pascal Frossard},
      year={2020},
      eprint={2010.09624},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{allenzhu2020feature,
      title={Feature Purification: How Adversarial Training Performs Robust Deep Learning}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2020},
      eprint={2005.10190},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wong2020fast,
      title={Fast is better than free: Revisiting adversarial training}, 
      author={Eric Wong and Leslie Rice and J. Zico Kolter},
      year={2020},
      eprint={2001.03994},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kim2020understanding,
      title={Understanding Catastrophic Overfitting in Single-step Adversarial Training}, 
      author={Hoki Kim and Woojin Lee and Jaewook Lee},
      year={2020},
      eprint={2010.01799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wu2020defending,
      title={Defending Against Physically Realizable Attacks on Image Classification}, 
      author={Tong Wu and Liang Tong and Yevgeniy Vorobeychik},
      year={2020},
      eprint={1909.09552},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.1145/2976749.2978392,
author = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
title = {Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978392},
doi = {10.1145/2976749.2978392},
abstract = {Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk.In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1528–1540},
numpages = {13},
keywords = {face recognition, neural networks, adversarial machine learning, face detection},
location = {Vienna, Austria},
series = {CCS '16}
}

@misc{eykholt2018robust,
      title={Robust Physical-World Attacks on Deep Learning Models}, 
      author={Kevin Eykholt and Ivan Evtimov and Earlence Fernandes and Bo Li and Amir Rahmati and Chaowei Xiao and Atul Prakash and Tadayoshi Kohno and Dawn Song},
      year={2018},
      eprint={1707.08945},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@misc{moosavidezfooli2016deepfool,
      title={DeepFool: a simple and accurate method to fool deep neural networks}, 
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
      year={2016},
      eprint={1511.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Sharif16AdvML,
  author =       {Mahmood Sharif and Sruti Bhagavatula and Lujo Bauer 
and Michael K. Reiter},
  title =        {Accessorize to a crime: {R}eal and stealthy attacks on 
state-of-the-art face recognition},
  booktitle =    {Proceedings of the 23rd ACM SIGSAC Conference on 
Computer and Communications Security},
  year =         2016,
  month =        oct,
  keywords =     {adversarial machine learning}
} 

@inproceedings{inproceedings,
author = {Pautov, Mikhail and Melnikov, Grigorii and Kaziakhmedov, Edgar and Kireev, Klim and Petyushko, Alexander},
year = {2019},
month = {10},
pages = {0391-0396},
title = {On Adversarial Patches: Real-World Attack on ArcFace-100 Face Recognition System},
doi = {10.1109/SIBIRCON48586.2019.8958134}
}

@misc{tramer2020ensemble,
      title={Ensemble Adversarial Training: Attacks and Defenses}, 
      author={Florian Tramer and Alexey Kurakin and Nicolas Papernot and Ian Goodfellow and Dan Boneh and Patrick McDaniel},
      year={2020},
      eprint={1705.07204},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}